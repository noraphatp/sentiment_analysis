{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract data from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports and downloads\n",
    "import os\n",
    "import nltk\n",
    "import math\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The production quality, cast, premise, authentic New England (Waterbury, CT?) locale and lush John Williams score should have resulted in a 3-4 star collectors item. Unfortunately, all we got was a passable 2 star \"decent\" flick, mostly memorable for what it tried to do.........bring an art house style film mainstream. The small town locale and story of ordinary people is a genre to itself, and if well done, will satisfy most grownups. Jane Fonda was unable to hide her braininess enough to make her character believable. I wondered why she wasn't doing a post doctorate at Yale instead of working in a dead end factory job in Waterbury. Robert DiNiro's character was just a bit too contrived. An illiterate, nice guy loser who turns out to actually be, with a little help from Jane's character, a 1990 version of Henry Ford or Thomas Edison.<br /><br />This genre has been more successfully handled by \"Nobody's Fool\" in the mid 90s and this year's (2003) \"About Schmidt.\" I wish that the main stream studios would try more stuff for post adolescents and reserve a couple of screens at the multi cinema complexes for those efforts.<br /><br />I'll give it an \"A\" for effort.\n",
      "The production quality, cast, premise, authentic New England (Waterbury, CT?) locale and lush John Williams score should have resulted in a 3-4 star collectors item. Unfortunately, all we got was a passable 2 star \"decent\" flick, mostly memorable for what it tried to do.........bring an art house style film mainstream. The small town locale and story of ordinary people is a genre to itself, and if well done, will satisfy most grownups. Jane Fonda was unable to hide her braininess enough to make her character believable. I wondered why she wasn't doing a post doctorate at Yale instead of working in a dead end factory job in Waterbury. Robert DiNiro's character was just a bit too contrived. An illiterate, nice guy loser who turns out to actually be, with a little help from Jane's character, a 1990 version of Henry Ford or Thomas Edison.<br /><br />This genre has been more successfully handled by \"Nobody's Fool\" in the mid 90s and this year's (2003) \"About Schmidt.\" I wish that the main stream studios would try more stuff for post adolescents and reserve a couple of screens at the multi cinema complexes for those efforts.<br /><br />I'll give it an \"A\" for effort.\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "def extract_review_score(folder_name):\n",
    "    documents = {}\n",
    "    scores = {}\n",
    "    id = 0\n",
    "\n",
    "    for sentiment in ('pos', 'neg'):\n",
    "        path = os.path.join(folder_name, sentiment)\n",
    "        \n",
    "        # sort filenames in order of ID\n",
    "        sorted_filenames = sorted(os.listdir(path), key=lambda x: int(x.split('_')[0])) \n",
    "\n",
    "        # loop in order of file names\n",
    "        for file_name in sorted_filenames:\n",
    "\n",
    "            # extract the score from filename \n",
    "            # ['{ID}','{score}.txt}'] -> ['{score}', 'txt'] -> '{score}'  \n",
    "            score = file_name.split('_')[1].split('.')[0]\n",
    "\n",
    "            # read content of the file\n",
    "            with open(os.path.join(path, file_name), 'r', encoding='utf-8') as file:\n",
    "                line = file.read()\n",
    "                documents[id] = line\n",
    "                scores[id] = 1 if int(score) >= 7 else 0\n",
    "                id += 1\n",
    "    \n",
    "    return documents, scores\n",
    "\n",
    "# extract data from 'train' and 'test' folders\n",
    "documents, doc_scores = extract_review_score('data')\n",
    "doc_reviews = list(documents.values())\n",
    "\n",
    "# print first review and its corresponding score\n",
    "print(documents.get(0))\n",
    "print(doc_reviews[0])\n",
    "print(doc_scores.get(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n",
      "4000\n",
      "4000\n"
     ]
    }
   ],
   "source": [
    "# both should be 4000 \n",
    "print(len(documents))\n",
    "print(len(doc_reviews))\n",
    "print(len(doc_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n",
      "The production quality, cast, premise, authentic New England (Waterbury, CT?) locale and lush John Williams score should have resulted in a 3-4 star collectors item. Unfortunately, all we got was a passable 2 star \"decent\" flick, mostly memorable for what it tried to do.........bring an art house style film mainstream. The small town locale and story of ordinary people is a genre to itself, and if well done, will satisfy most grownups. Jane Fonda was unable to hide her braininess enough to make her character believable. I wondered why she wasn't doing a post doctorate at Yale instead of working in a dead end factory job in Waterbury. Robert DiNiro's character was just a bit too contrived. An illiterate, nice guy loser who turns out to actually be, with a little help from Jane's character, a 1990 version of Henry Ford or Thomas Edison.<br /><br />This genre has been more successfully handled by \"Nobody's Fool\" in the mid 90s and this year's (2003) \"About Schmidt.\" I wish that the main stream studios would try more stuff for post adolescents and reserve a couple of screens at the multi cinema complexes for those efforts.<br /><br />I'll give it an \"A\" for effort.\n",
      "[('production',), ('quality',), ('cast',), ('premise',), ('authentic',), ('new',), ('england',), ('waterbury',), ('ct',), ('locale',), ('lush',), ('john',), ('williams',), ('score',), ('resulted',), ('3-4',), ('star',), ('collector',), ('item',), ('unfortunately',), ('got',), ('passable',), ('2',), ('star',), ('decent',), ('flick',), ('mostly',), ('memorable',), ('tried',), ('ing',), ('art',), ('house',), ('style',), ('film',), ('mainstream',), ('small',), ('town',), ('locale',), ('story',), ('ordinary',), ('people',), ('genre',), ('well',), ('done',), ('satisfy',), ('grownup',), ('jane',), ('fonda',), ('unable',), ('hide',), ('aininess',), ('enough',), ('make',), ('character',), ('believable',), ('wondered',), (\"n't\",), ('post',), ('doctorate',), ('yale',), ('instead',), ('working',), ('dead',), ('end',), ('factory',), ('job',), ('waterbury',), ('robert',), ('diniro',), ('character',), ('bit',), ('contrived',), ('illiterate',), ('nice',), ('guy',), ('loser',), ('turn',), ('actually',), ('little',), ('help',), ('jane',), ('character',), ('1990',), ('version',), ('henry',), ('ford',), ('thomas',), ('edison.',), ('genre',), ('successfully',), ('handled',), ('nobody',), ('fool',), ('mid',), ('90',), ('year',), ('2003',), ('schmidt',), ('wish',), ('main',), ('stream',), ('studio',), ('would',), ('try',), ('stuff',), ('post',), ('adolescent',), ('reserve',), ('couple',), ('screen',), ('multi',), ('cinema',), ('complex',), ('efforts.',), (\"'ll\",), ('give',), ('effort',)]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# do feature generation after the data split\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Features 1\n",
    "# - lowercase\n",
    "# - remove stopwords and punctuation\n",
    "# - use lemmatization\n",
    "\n",
    "def generate_features_1(doc_reviews, n):\n",
    "    review_ngrams = {}\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for id, review in enumerate(doc_reviews):\n",
    "        word_list = nltk.word_tokenize(review.lower()) \n",
    "        word_list = [lemmatizer.lemmatize(word) for word in word_list if not word in stop_words and not word in string.punctuation]\n",
    "\n",
    "        # remove \"''\", 'br','``','--','...',\"'s\" using .replace()\n",
    "        word_list = [word.replace(\"''\", \"\").replace('br', \"\").replace('``', \"\").replace('--', \"\").replace('...', \"\").replace(\"'s\", \"\") for word in word_list]\n",
    "        # remove empty strings\n",
    "        word_list = [word for word in word_list if word != \"\"]\n",
    "\n",
    "        n_grams = list(nltk.ngrams(word_list, n))\n",
    "        review_ngrams[id] = n_grams # add to dictionary with corresponding review ID (same numbering as the text files)\n",
    "\n",
    "    return review_ngrams\n",
    "\n",
    "# Features 2\n",
    "# - remove stopwords and punctuation\n",
    "# - use stemming\n",
    "def generate_features_2(doc_reviews, n):\n",
    "    review_ngrams = {}\n",
    "    st = LancasterStemmer() \n",
    "    for id, review in enumerate(doc_reviews):\n",
    "        word_list = nltk.word_tokenize(review) # tokenize\n",
    "        word_list = [st.stem(word) for word in word_list if not word in stop_words and not word in string.punctuation] \n",
    "\n",
    "        word_list = [word.replace(\"''\", \"\").replace('br', \"\").replace('``', \"\").replace('--', \"\").replace('...', \"\").replace(\"'s\", \"\") for word in word_list]\n",
    "\n",
    "        n_grams = list(nltk.ngrams(word_list, n))\n",
    "        review_ngrams[id] = n_grams \n",
    "\n",
    "    return review_ngrams\n",
    "\n",
    "# Features 3\n",
    "# - only lemmatisation used\n",
    "def generate_features_3(doc_reviews, n):\n",
    "    review_ngrams = {}\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for id, review in enumerate(doc_reviews):\n",
    "        word_list = nltk.word_tokenize(review) # tokenize\n",
    "        word_list = [lemmatizer.lemmatize(word) for word in word_list] \n",
    "\n",
    "        word_list = [word.replace(\"''\", \"\").replace('br', \"\").replace('``', \"\").replace('--', \"\").replace('...', \"\").replace(\"'s\", \"\") for word in word_list]\n",
    "\n",
    "        n_grams = list(nltk.ngrams(word_list, n))\n",
    "        review_ngrams[id] = n_grams \n",
    "\n",
    "    return review_ngrams\n",
    "\n",
    "\n",
    "n_value = 1 # change for other n-grams\n",
    "doc_features = generate_features_1(doc_reviews, n_value) # we will be testing with features 1, 2 and 3. i will use one at a time for simplicity\n",
    "# doc_features = generate_features_2(doc_reviews, n_value)\n",
    "# doc_features = generate_features_3(doc_reviews, n_value)\n",
    "print(len(doc_features))\n",
    "print(doc_reviews[0])\n",
    "print(doc_features.get(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40609\n"
     ]
    }
   ],
   "source": [
    "all_terms = []\n",
    "for id, reviews in doc_features.items():\n",
    "    for terms in reviews:\n",
    "        all_terms.append(terms)\n",
    "\n",
    "all_terms = list(set(all_terms)) # remove duplicates\n",
    "print(len(all_terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio: 0.5\n",
      "ratio: 0.5\n",
      "ratio: 0.5\n",
      "[1006, 619, 20]\n",
      "I believe this is the most powerful film HBO Pictures has made to date. This film should have been released in theaters for the public to view on the big screen. It is available on video so make sure you look for it and check it out. Chris Gerolmo did a great job with the direction and the screenplay. The performances from Stephen Rea, Donald Sutherland and Jeffery DeMunn are flawless. A masterpiece of the genre.\n",
      "[('believe',), ('powerful',), ('film',), ('hbo',), ('picture',), ('made',), ('date',), ('film',), ('released',), ('theater',), ('public',), ('view',), ('big',), ('screen',), ('available',), ('video',), ('make',), ('sure',), ('look',), ('check',), ('chris',), ('gerolmo',), ('great',), ('job',), ('direction',), ('screenplay',), ('performance',), ('stephen',), ('rea',), ('donald',), ('sutherland',), ('jeffery',), ('demunn',), ('flawless',), ('masterpiece',), ('genre',)]\n",
      "[(1671, 1), (1897, 1), (605, 1), (1101, 1), (1906, 1), (1570, 1), (1575, 1), (1199, 1), (300, 1), (407, 1)]\n",
      "[[('disney',), ('film',), ('name',), ('stood',), ('thing',), ('innocent',), ('suitable',), ('age',), ('finally',), ('started',), ('realise',), ('survive',), ('need',), ('become',), ('diverse',), ('diversity',), ('apparent',), ('last',), ('couple',), ('year',), ('film',), ('like',), ('tarzan',), ('emperor',), ('new',), ('groove',), ('made',), ('attempt',), ('move',), ('away',), ('traditional',), ('song-driven',), ('routine',), ('disney',), ('past',), ('new',), ('uncharted',), ('territory',), ('atlantis',), ('boldest',), ('step',), ('yet',), ('remember',), ('still',), ('disney',), ('first',), ('ever',), ('serious',), ('film',), ('come',), ('disney',), ('animation',), ('studio',), ('major',), ('achievement',), ('fact',), ('serious',), ('make',), ('pg',), ('territory',), ('perhaps',), ('lot',), ('family',), ('scared',), ('seeing',), ('past',), ('summer.',), ('despite',), ('mature',), ('subject',), ('matter',), ('still',), ('film',), ('disney',), ('wanted',), ('draw',), ('family',), ('mature',), ('audience',), ('plot',), ('kept',), ('simple',), ('enough',), ('child',), ('understand',), ('interesting',), ('enough',), ('take',), ('away',), ('realm',), ('little',), ('mermaid',), ('et',), ('al.',), ('get',), ('actually',), ('potentially',), ('detailed',), ('plot',), ('unfortunately',), ('suffering',), ('blow',), ('condensed',), ('96-minute',), ('movie',), ('ultimately',), ('action',), ('film',), ('atlantis',), ('exposition',), ('preceding',), ('whisked',), ('first',), ('half',), ('hour',), ('many',), ('sequence',), ('bombarding',), ('screen',), ('possible',), ('without',), ('losing',), ('coherency',), ('suspend',), ('disbelief',), ('character',), ('get',), ('point',), ('point',), ('b',), ('quickly',), (\"'re\",), ('unlikely',), ('find',), ('animated',), ('film',), ('detailed',), ('coming',), ('hollywood',), ('want',), ('epic',), ('level',), ('detail',), ('plot',), ('turn',), ('james',), ('cameron',), ('titanic',), ('film',), ('feature',), ('boat',), ('manner.',), ('let',), ('talk',), ('love',), ('shall',), ('yes',), ('lot',), ('film',), ('lead',), ('male',), ('one',), ('milo',), ('thatch',), ('bumbling',), ('archaeologist',), ('lead',), ('female',), ('kida',), ('clichéd',), ('atlantian',), ('princess',), ('set',), ('fall',), ('love',), ('found',), ('clichéd',), ('expecting',), ('film',), ('end',), ('character',), ('touching/feeling/kissing',), ('sequence',), ('far',), ('subdued',), ('various',), ('point',), ('film',), ('attraction',), ('grows',), ('ballpark',), ('say',), ('little',), ('mermaid',), ('good',), ('thing',), ('may',), ('grasped',), ('rather',), ('clichéd',), ('film',), ('correct',), ('leading',), ('hero',), ('heroine',), ('backed',), ('half',), ('dozen',), ('crew',), ('member',), ('go',), ('expedition',), ('given',), ('moment',), ('film',), ('numerous',), ('character',), ('appear',), ('take',), ('minute',), ('screentime',), ('disappear',), (\"n't\",), ('take',), ('genius',), ('math',), ('\\x96',), ('96-minute',), ('film',), ('focus',), ('action',), ('visuals',), ('considerable',), ('cast',), ('little',), ('time',), ('expand',), ('character',), ('major',), ('extent',), ('rely',), ('clichés',), ('lot',), ('every',), ('character',), ('emulates',), ('something',), ('done',), ('thousand',), ('time',), ('bumbling',), ('scientist',), ('attractive',), ('princess',), ('square-jawed',), ('colonel',), ('rich',), ('eccentric',), ('maniacal',), ('sleazebag',), ('russian',), ('femme',), ('fatale',), ('\\x96',), ('need',), ('go',), (\"n't\",), ('know',), ('got',), ('anyone',), ('\\x96',), ('found',), ('tongue-in-cheek',), ('nature',), ('film',), ('quite',), ('amusing',), ('alright',), ('meant',), ('serious',), ('flick',), ('really',), ('expect',), ('disney',), ('give',), ('every',), ('single',), ('trait',), ('history',), ('least',), ('writer',), ('tried',), ('come',), ('consistently',), ('witty',), ('dialogue',), ('sometimes',), ('even',), ('little',), ('inspired.',), ('end',), ('big',), ('stunning',), ('visuals',), ('put',), ('icing',), ('cake',), ('cgi',), ('animation',), ('truly',), ('amazing',), ('place',), (\"n't\",), ('dwarf',), ('character',), ('flaw',), ('let',), ('recent',), ('titan',), ('a.e',), ('speaking',), ('character',), ('disney',), ('hired',), ('outside',), ('comic',), ('industry',), ('artist',), ('create',), ('design',), ('inging',), ('anime',), ('style',), ('film',), ('infact',), ('visual',), ('presentation',), ('film',), ('whole',), ('owes',), ('lot',), ('anime',), ('much',), ('previous',), ('disney',), ('outing',), ('resulted',), ('conflict',), ('fan',), ('japanese',), ('anime',), ('nadia',), ('film',), ('overall',), ('similarity',), ('said',), ('cartoon',), ('series',), ('seen',), ('anime',), ('ca',), (\"n't\",), ('comment.',), ('picture',), ('sound',), ('gary',), ('rydstrom',), ('head',), ('sound',), ('team',), ('soundtrack',), ('opening',), ('shot',), ('sound',), ('stage',), ('alive',), ('treat',), ('james',), ('newton',), ('howard',), ('treat',), ('u',), ('dynamic',), ('musical',), ('score',), ('compliment',), ('film',), ('every',), ('way',), ('never',), ('sounding',), ('place',), ('always',), ('helping',), ('build',), ('tension',), ('subdue',), ('it.',), ('perhaps',), ('missed',), ('point',), ('creator',), ('intended',), ('film',), ('conveys',), ('adventure',), ('thrill',), ('ride',), ('albeit',), ('serious',), ('tone',), ('disney',), ('film',), (\"n't\",), ('like',), ('clichéd',), ('tongue-in-cheek',), ('attitude',), ('perhaps',), ('effort',), ('poured',), ('visuals',), ('delight',), ('heck',), ('least',), ('mythology',), ('far',), ('correct',), ('said',), ('disney',), ('effort',), ('cough',), ('hercules',), ('cough',), ('positive',), ('10',), ('10',), ('review',), ('someone',), ('blown',), ('away',), ('film',), ('always',), ('suspend',), ('disbelief',), ('animated',), ('film',), ('\\x96',), ('law',), ('real',), ('world',), ('frequently',), ('oken',), ('cartoon',), ('medium',), ('sit',), ('back',), ('enjoy',), ('ride',), ('perhaps',), ('everyone',), ('find',), ('something',), ('enjoy',), ('film',)], [('director',), ('paul',), ('verhoeven',), ('american',), ('vehicle',), ('varied',), ('quality',), ('film',), ('made',), ('native',), ('country',), ('indisputable',), ('masterworks',), ('story',), ('alcoholic',), ('bi-sexual',), ('writer',), ('move',), ('beautiful',), ('rich',), ('strange',), ('woman',), ('lady',), ('know',), ('interested',), ('meeting',), ('woman',), ('handsome',), ('male',), ('lover',), ('meantime',), ('writer',), ('plagued',), ('strange',), ('vision',), ('first',), ('look',), ('like',), ('hallucination',), ('triggered',), ('alcohol',), ('abuse',), ('soon',), ('begin',), ('realize',), ('actually',), ('experiencing',), ('kind',), ('premonition',), ('fascinating',), ('hitchcockian',), ('thriller',), ('original',), ('provocative',), ('love',), ('film',), ('make',), ('think',), ('something',), ('realize',), ('something',), ('completely',), ('different',), ('one',), ('movie',), ('thriller',), ('first',), ('half',), ('quasi-religious',), ('surrealist',), ('saga',), ('second',), ('half',), ('erotic',), ('original',), ('blasphemous',), ('kid',), ('people',), ('go',), ('church',), ('every',), ('sunday',), ('great',), ('cinematography',), ('future',), ('director',), ('jan',), ('de',), ('bont',), ('highly',), ('reommended',)], [('contains',), ('spoiler',), ('due',), ('describe',), ('film',), ('technique',), ('read',), ('risk',), ('loved',), ('film',), ('use',), ('tinting',), ('scene',), ('make',), ('seem',), ('like',), ('old',), ('photograph',), ('come',), ('life',), ('also',), ('enjoyed',), ('projection',), ('people',), ('back',), ('screen',), ('instance',), ('one',), ('scene',), ('leopold',), ('call',), ('wife',), ('projected',), ('behind',), ('rather',), ('typical',), ('split',), ('screen',), ('face',), ('huge',), ('back',), ('leo',), ('foreground.',), ('one',), ('best',), ('us',), ('young',), ('boy',), ('kill',), ('ravensteins',), ('train',), ('scene',), ('shot',), ('almost',), ('political',), ('poster',), ('style',), ('facial',), ('close',), ('ups',), ('reminded',), ('battleship',), ('potemkin',), ('intense',), ('constant',), ('style',), ('coupled',), ('spray',), ('red',), ('convey',), ('ton',), ('horror',), ('without',), ('much',), ('gore',), ('scene',), ('katharina',), ('find',), ('father',), ('dead',), ('bathtub',), ('see',), ('red',), ('water',), ('side',), ('one',), ('thing',), ('love',), ('von',), ('trier',), ('understatement',), ('horror',), ('end',), ('making',), ('creepy.',), ('use',), ('text',), ('film',), ('unique',), ('like',), ('leo',), ('character',), ('pushed',), ('word',), ('werewolf',), ('never',), ('seen',), ('anything',), ('like',), ('film.',), ('use',), ('black',), ('comedy',), ('film',), ('well',), ('done',), ('ernst-hugo',), ('järegård',), ('great',), ('leo',), ('uncle',), ('ings',), ('snicker',), ('got',), ('role',), ('kingdom',), ('riget',), ('humor',), ('make',), ('plotline',), ('absurd',), ('anal',), ('retentiveness',), ('train',), ('conductor',), ('terrible',), ('backdrop',), ('ww2',), ('chaos',), ('easier',), ('take',), ('reminds',), ('riget',), ('way',), ('hospital',), ('administrator',), ('trying',), ('maintain',), ('normalcy',), ('end',), ('part',), ('one',), ('everything',), ('going',), ('crazy',), ('show',), ('people',), ('truly',), ('oblivious',), ('awful',), ('thing',), ('happening',), ('around',), ('yet',), ('people',), ('like',), ('leo',), ('tuned',), ('nothing',), ('positive',), ('it.',), ('voice',), ('done',), ('expertly',), ('well',), ('max',), ('von',), ('sydow',), ('amusing',), ('draw',), ('story',), ('make',), ('jump',), ('leo',), ('head',), ('time',), ('scary',), ('place',), ('be.',), ('movie',), ('ings',), ('point',), ('one',), ('coward',), (\"n't\",), ('choose',), ('side',), ('see',), ('idea',), ('used',), ('dancer',), ('dark',), ('bjork',), ('character',), (\"n't\",), ('speak',), ('end',), ('destruction',), ('actually',), ('one',), ('time',), ('von',), ('trier',), ('seemed',), ('anti-woman',), ('making',), ('eaking',), ('wave',), ('dancer',), ('know',), ('male',), ('character',), (\"n't\",), ('fare',), ('well',), ('either',), ('found',), ('place',), ('end',), ('dancer',), ('seriously',), ('want',), ('main',), ('character',), ('rethink',), ('action',), ('course',), ('never',)], [('war',), ('inc.',), ('2008',), ('1/2',), ('john',), ('cusack',), ('marisa',), ('tomei',), ('hilary',), ('duff',), ('joan',), ('cusack',), ('ben',), ('kingsley',), ('dan',), ('aykroyd',), ('sergej',), ('trifunovic',), ('lyubomir',), ('neikov',), ('ned',), ('bellamy',), ('voice',), ('montel',), ('williams',), ('hit-and-miss-21st',), ('century',), ('strangelove',), ('john',), ('cusack',), ('\\x96',), ('co-wrote',), ('script',), ('mark',), ('leyner',), ('jeremy',), ('pikser',), ('\\x96',), ('star',), ('jaded',), ('hit-man',), ('named',), ('and',), ('hauser',), ('burnt',), ('decides',), ('follow',), ('thru',), ('one',), ('final',), ('assignment',), ('icing',), ('middle-eastern',), ('oil',), ('minister',), ('named',), ('omar',), ('sharif',), ('yes',), ('omar',), ('sharif',), ('get',), ('tone',), ('misfire',), ('laugh',), ('commandeered',), ('ex-vice',), ('president',), ('usa',), ('aykroyd',), ('cusack',), ('old',), ('grosse',), ('pointe',), ('blank',), ('co-hort',), ('mean',), ('dick',), ('cheney',), ('manqué',), ('turn',), ('enlisting',), ('and',), ('deed',), ('guise',), ('trade',), ('show',), ('producer',), ('mythical',), ('turaqistan',), ('read',), ('iraq/afghanistan',), ('american',), ('private',), ('corporation',), ('tamerlane',), ('read',), ('halliburton',), ('iefed',), ('and',), ('faced',), ('moment',), ('clarity',), ('come',), ('across',), ('intrepid',), ('journalist',), ('natalie',), ('hegalhuzen',), ('tomei',), ('eventually',), ('fall',), ('love',), ('meanwhile',), ('tamerlane',), ('sponsoring',), ('unlikely',), ('union',), ('eastern',), ('european',), ('teen',), ('sensation',), ('yonica',), ('babyyeah',), ('surprisingly',), ('decent',), ('duff',), ('aping',), ('celeity',), ('tongue-',), ('through-cheek',), ('idiot',), ('son',), ('country',), ('leader.',), ('follows',), ('bold',), ('attempt',), ('21st',), ('century',), ('black',), ('comedy',), ('la',), ('dr.',), ('strangelove',), ('intense',), ('purpose',), ('sadly',), ('miss',), ('hit',), ('oad',), ('try',), ('laugh',), ('amidst',), ('political',), ('message',), ('unjust',), ('war',), ('outsourced',), ('american',), ('capitalism',), ('check',), ('cusack',), ('riff',), ('martin',), ('blank',), ('aforementioned',), ('pointe',), ('add',), ('nice',), ('touch',), ('man',), ('black',), ('shot',), ('tabasco',), ('sauce',), ('take',), ('edge',), ('rest',), ('cast',), ('play',), ('catch',), ('except',), ('sister',), ('joan',), ('riot',), ('high-strung',), ('aide-',), ('de-camp',), ('hauser',), ('one',), ('film',), ('funniest',), ('laugh-out',), ('line',), ('mass',), ('communication',), ('skill',), ('finally',), ('paying',), ('part',), ('cusack',), ('visited',), ('iraq',), ('war',), ('earlier',), ('year',), ('180',), ('degree',), ('different',), ('grace',), ('gone',), ('allows',), ('political',), ('view',), ('wear',), ('sleeve',), ('admirable',), ('overall',), ('film',), ('pace',), ('rhythm',), ('largely',), ('no-thanks',), ('first',), ('time',), ('filmmaker',), ('joshua',), ('seftel',), ('making',), ('directorial',), ('debut',), ('noticeable',), ('except',), ('maybe',), ('well-choreographed',), ('fight',), ('hauser',), ('involved',), ('babyyeah',), ('idiotic',), ('fiancé',), ('entourage.',), ('nice',), ('attempt',), ('yet',), ('misguided',), ('failure',), ('maybe',), ('next',), ('time',), ('cusack',), ('wo',), (\"n't\",), ('try',), ('hard',), ('let',), ('idiocy',), ('war',), ('speak',), ('instead',), ('heavy',), ('lifting',)], [('interesting',), ('animation',), ('fate',), ('giant',), ('tiger',), ('sloth',), ('mammoth',), ('saved',), ('baby',), ('close',), ('killed',), ('group',), ('tiger',), ('ice',), ('age',), ('morale',), ('film',), ('show',), ('good',), ('behavior',), ('others',), ('may',), ('ing',), ('benefit',), ('end',), ('one',), ('tiger',), ('group',), ('got',), ('order',), ('finally',), ('capture',), ('baby',), ('hardly',), ('saved',), ('mother',), ('tiger',), ('attacked',), ('community',), ('baby',), ('rescued',), ('sloth',), ('mammoth',), ('tiger',), ('joined',), ('objective',), ('finally',), ('taken',), ('away',), ('baby',), ('went',), ('troublesome',), ('path',), ('plenty',), ('danger',), ('tiger',), ('fall',), ('saved',), ('mammoth',), ('end',), ('group',), ('tiger',), ('tried',), ('capture',), ('baby',), ('mammoth',), ('helped',), ('incredibly',), ('tiger',), ('colleague',), ('able',), ('overcome',), ('attack',), ('give',), ('baby',), ('back',), ('father',), ('community',), ('belongs',)]]\n"
     ]
    }
   ],
   "source": [
    "random.seed(3)\n",
    "\n",
    "# split data into training, validation and testings\n",
    "num_train_docs = int(len(documents) * 0.7 * 0.5) # ensure equal number of positive and negative reviews for training\n",
    "num_validation_docs = int(len(documents) * 0.2 * 0.5)\n",
    "num_test_docs = int(len(documents) * 0.1 * 0.5)\n",
    "\n",
    "# shuffle the IDs\n",
    "pos_index = list(range(len(doc_features) // 2))\n",
    "neg_index = list(range(len(doc_features) // 2, len(doc_features)))\n",
    "\n",
    "# randomize indices\n",
    "random.shuffle(pos_index)\n",
    "random.shuffle(neg_index)\n",
    "\n",
    "# split the IDs\n",
    "train_doc_ids = pos_index[:num_train_docs] + neg_index[:num_train_docs]\n",
    "validation_doc_ids = pos_index[num_train_docs: num_train_docs + num_validation_docs] + neg_index[num_train_docs: num_train_docs + num_validation_docs]\n",
    "test_doc_ids = pos_index[num_train_docs + num_validation_docs:] + neg_index[num_train_docs + num_validation_docs:]\n",
    "\n",
    "# split the reviews\n",
    "train_reviews = {id: doc_features[id] for id in train_doc_ids}\n",
    "validation_reviews = {id: doc_features[id] for id in validation_doc_ids}\n",
    "test_reviews = {id: doc_features[id] for id in test_doc_ids}\n",
    "\n",
    "# split the scores\n",
    "train_scores = {id: doc_scores[id] for id in train_doc_ids}\n",
    "validation_scores = {id: doc_scores[id] for id in validation_doc_ids}\n",
    "test_scores = {id: doc_scores[id] for id in test_doc_ids}\n",
    "\n",
    "print(f\"ratio: {sum(train_scores.values())/len(train_scores)}\") \n",
    "print(f\"ratio: {sum(validation_scores.values())/len(validation_scores)}\")\n",
    "print(f\"ratio: {sum(test_scores.values())/len(test_scores)}\")\n",
    "\n",
    "print(list(train_reviews.keys())[:3])\n",
    "print(documents.get(1823))\n",
    "print(train_reviews.get(1823))\n",
    "print(list(validation_scores.items())[:10])\n",
    "\n",
    "print(list(train_reviews.values())[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('believe',): 1, ('powerful',): 1, ('film',): 2, ('hbo',): 1, ('picture',): 1, ('made',): 1, ('date',): 1, ('released',): 1, ('theater',): 1, ('public',): 1, ('view',): 1, ('big',): 1, ('screen',): 1, ('available',): 1, ('video',): 1, ('make',): 1, ('sure',): 1, ('look',): 1, ('check',): 1, ('chris',): 1, ('gerolmo',): 1, ('great',): 1, ('job',): 1, ('direction',): 1, ('screenplay',): 1, ('performance',): 1, ('stephen',): 1, ('rea',): 1, ('donald',): 1, ('sutherland',): 1, ('jeffery',): 1, ('demunn',): 1, ('flawless',): 1, ('masterpiece',): 1, ('genre',): 1}\n"
     ]
    }
   ],
   "source": [
    "def calc_normalized_tf(reviews):\n",
    "    tf = {} \n",
    "\n",
    "    for id, review in reviews.items(): # go through each review\n",
    "        tf[id] = {}\n",
    "        for word in review:\n",
    "            if word not in tf[id]:\n",
    "                tf[id][word] = 1 # if word not in dictionary, initialize to 1. \n",
    "            else:\n",
    "                tf[id][word] += 1 # if word in dictionary, increment by 1\n",
    "\n",
    "    return tf\n",
    "\n",
    "term_freq = calc_normalized_tf(train_reviews)\n",
    "print(term_freq.get(1823))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PPMI\n",
    "\n",
    "#construct term co-occurence matrix\n",
    "# def construct_term_to_term_matrix(reviews):\n",
    "#     tot_mat = {}\n",
    "\n",
    "#     for id, review in reviews.items():\n",
    "#         words = set(review) # get unique words in the review\n",
    "#         for word in words:\n",
    "#             if word not in tot_mat:\n",
    "#                 tot_mat[word] = {}\n",
    "#             for other_word in words:\n",
    "#                 if other_word != word:\n",
    "#                     if other_word not in tot_mat[word]:\n",
    "#                         tot_mat[word][other_word] = 1 # if the word pair does not exist, add it to the matrix\n",
    "#                     else:\n",
    "#                         tot_mat[word][other_word] += 1 # if the word pair exists, increment the count by 1\n",
    "\n",
    "#     return tot_mat\n",
    "\n",
    "# def calc_ppmi(tot_mat):\n",
    "#     ppmi = {}\n",
    "\n",
    "#     total_count = sum(sum(counts.values()) for counts in tot_mat.values())\n",
    "#     word_count = {word: sum(counts.values()) for word, counts in tot_mat.items()} \n",
    "\n",
    "#     for word, counts in tot_mat.items():\n",
    "#         ppmi[word] = {}\n",
    "        \n",
    "#         for other_word, count in counts.items(): # loop through each word pair\n",
    "#             # calculate PPMI using the formula\n",
    "#             p_x = word_count[word] / total_count \n",
    "#             p_y = word_count[other_word] / total_count\n",
    "#             p_xy = count / total_count\n",
    "#             ppmi[word][other_word] = max(math.log(p_xy / (p_x * p_y), 2), 0)\n",
    "    \n",
    "#     return ppmi\n",
    "\n",
    "# tot_mat = construct_term_to_term_matrix(train_reviews)\n",
    "# train_ppmi = calc_ppmi(tot_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1691\n",
      "0.21901442374447744\n",
      "I believe this is the most powerful film HBO Pictures has made to date. This film should have been released in theaters for the public to view on the big screen. It is available on video so make sure you look for it and check it out. Chris Gerolmo did a great job with the direction and the screenplay. The performances from Stephen Rea, Donald Sutherland and Jeffery DeMunn are flawless. A masterpiece of the genre.\n",
      "{('believe',): 1.0272022828524614, ('powerful',): 1.6276140958003502, ('film',): 0.4380288474889549, ('hbo',): 2.3679767852945943, ('picture',): 1.1396619934290062, ('made',): 0.6152882570617174, ('date',): 1.6912831756697277, ('released',): 1.4603862970759742, ('theater',): 1.4740301777425204, ('public',): 1.6837300377792819, ('view',): 1.4259687322722812, ('big',): 0.9420080530223132, ('screen',): 1.0724096853321152, ('available',): 1.9030899869919433, ('video',): 1.3298867356864548, ('make',): 0.4475925431162368, ('sure',): 1.045757490560675, ('look',): 0.7020832397601616, ('check',): 1.583835171221763, ('chris',): 1.9156791142999638, ('gerolmo',): 3.447158031342219, ('great',): 0.5767541260631921, ('job',): 1.0742460283721125, ('direction',): 1.2296740871283127, ('screenplay',): 1.6618281963314518, ('performance',): 0.8343741746224836, ('stephen',): 1.8789563072752242, ('rea',): 2.970036776622557, ('donald',): 2.1249387366083, ('sutherland',): 2.3679767852945943, ('jeffery',): 2.970036776622557, ('demunn',): 3.1461280356782377, ('flawless',): 2.271066772286538, ('masterpiece',): 1.6146491186359828, ('genre',): 1.353736346179984}\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "\n",
    "# Document frequency (how many documents contain a term)\n",
    "def calc_df(reviews):\n",
    "    df = {}\n",
    "    for id, review in reviews.items():\n",
    "        for word in set(review):\n",
    "            if word not in df:\n",
    "                df[word] = 1\n",
    "            else:\n",
    "                df[word] += 1\n",
    "\n",
    "    return df\n",
    "\n",
    "doc_freq = calc_df(train_reviews)\n",
    "print(doc_freq.get(('film',)))\n",
    "\n",
    "# Inverse document frequency (how important a term is)\n",
    "def calc_idf(df, num_docs):\n",
    "    idf = {}\n",
    "    for word, term_df in df.items():\n",
    "        idf[word] = math.log(num_docs / term_df, 10)\n",
    "\n",
    "    return idf\n",
    "\n",
    "inverse_doc_freq = calc_idf(doc_freq, len(train_reviews))\n",
    "print(inverse_doc_freq.get(('film',)))\n",
    "\n",
    "# TF-IDF (term frequency - inverse document frequency)\n",
    "def calc_tfidf(tf, idf):\n",
    "    tfidf = {}\n",
    "    for id, review in tf.items():\n",
    "        tfidf[id] = {}\n",
    "        for word, freq in review.items():\n",
    "            tfidf[id][word] = freq * idf[word]\n",
    "\n",
    "    return tfidf\n",
    "\n",
    "train_tfidf = calc_tfidf(term_freq, inverse_doc_freq)\n",
    "\n",
    "# print first review and its corresponding TF-IDF\n",
    "print(doc_reviews[1823])\n",
    "print(train_tfidf.get(1823))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naïve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for validation set: 0.775\n",
      "accuracy for test set: 0.7825\n"
     ]
    }
   ],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    def __init__(self):\n",
    "        self.positive = {} # P(feature | positive)\n",
    "        self.negative = {} # P(feature | negative)\n",
    "        self.positive_prior = 0 # P(positive)\n",
    "        self.negative_prior = 0 # P(negative)  \n",
    "\n",
    "    def train_tfidf(self, train_tfidf, train_scores):\n",
    "        # calculate P(\"positive\") and P(\"negative\"), P(content | \"positive\"), P(content | \"negative\")\n",
    "        pos_id = []\n",
    "        neg_id = []\n",
    "\n",
    "        for id in train_tfidf.keys(): # separate positive and negative IDs\n",
    "            if train_scores.get(id) == 1:\n",
    "                pos_id.append(id)\n",
    "            else:\n",
    "                neg_id.append(id)     \n",
    "        \n",
    "        self.positive_prior = len(pos_id) / len(train_tfidf.keys()) # P(positive)\n",
    "        self.negative_prior = len(neg_id) / len(train_tfidf.keys()) # P(negative)\n",
    "\n",
    "        positive_total_tfidf = 0 \n",
    "        negative_total_tfidf = 0\n",
    "\n",
    "        for id, sub_dict in train_tfidf.items():\n",
    "            for term, tfidf_value in sub_dict.items():\n",
    "                if id in pos_id:\n",
    "                    positive_total_tfidf += tfidf_value\n",
    "                    if term not in self.positive:\n",
    "                        self.positive[term] = tfidf_value\n",
    "                    else:\n",
    "                        self.positive[term] += tfidf_value\n",
    "                else:\n",
    "                    negative_total_tfidf += tfidf_value\n",
    "                    if term not in self.negative:\n",
    "                        self.negative[term] = tfidf_value\n",
    "                    else:\n",
    "                        self.negative[term] += tfidf_value\n",
    "                \n",
    "\n",
    "        # divide by total tf-idf value for each class\n",
    "        for term in self.positive.keys():\n",
    "            self.positive[term] /= positive_total_tfidf\n",
    "        for term in self.negative.keys():\n",
    "            self.negative[term] /= negative_total_tfidf\n",
    "\n",
    "    \n",
    "    def train_ppmi(self, train_ppmi, train_scores, train_reviews):\n",
    "        pos_id = []\n",
    "        neg_id = []\n",
    "\n",
    "        for id in train_reviews.keys():\n",
    "            if train_scores.get(id) == 1:\n",
    "                pos_id.append(id)\n",
    "            else:\n",
    "                neg_id.append(id)     \n",
    "        \n",
    "        self.positive_prior = len(pos_id) / len(train_reviews.keys())\n",
    "        self.negative_prior = len(neg_id) / len(train_reviews.keys())\n",
    "\n",
    "        # initialize all values to 0\n",
    "        self.positive = {word: 0 for word in train_ppmi} \n",
    "        self.negative = {word: 0 for word in train_ppmi}\n",
    "\n",
    "        for id in pos_id: # for each positive review \n",
    "            for word in train_reviews[id]:  \n",
    "                if word in self.positive:\n",
    "                    # sum up the PPMI values for each existing word pair in the review\n",
    "                    self.positive[word] += sum(train_ppmi[word].get(other_word, 0) for other_word in train_reviews[id] if other_word in train_ppmi[word])\n",
    "\n",
    "        for id in neg_id:\n",
    "            for word in train_reviews[id]:\n",
    "                if word in self.negative:\n",
    "                    self.negative[word] += sum(train_ppmi[word].get(other_word, 0) for other_word in train_reviews[id] if other_word in train_ppmi[word])\n",
    "        \n",
    "        total_positive = sum(self.positive.values())\n",
    "        total_negative = sum(self.negative.values())\n",
    "\n",
    "        # divide by total PPMI value for each class\n",
    "        for word in self.positive.keys():\n",
    "            self.positive[word] /= total_positive\n",
    "        for word in self.negative.keys():\n",
    "            self.negative[word] /= total_negative\n",
    "\n",
    "        # set all 0 values to 1e-10 to avoid log(0)\n",
    "        for word in self.positive.keys():\n",
    "            if self.positive[word] <= 0:\n",
    "                self.positive[word] = 1e-10\n",
    "        \n",
    "        for word in self.negative.keys():\n",
    "            if self.negative[word] <= 0:\n",
    "                self.negative[word] = 1e-10\n",
    "        \n",
    "                \n",
    "    def predict(self, reviews):\n",
    "        classification = {}\n",
    "\n",
    "        for id, review in reviews.items():\n",
    "            p_positive = math.log(self.positive_prior)\n",
    "            p_negative = math.log(self.negative_prior)\n",
    "\n",
    "            for term in review:\n",
    "                if term in self.positive:\n",
    "                    p_positive += math.log(self.positive[term])\n",
    "                else:  # Laplace smoothing for positive\n",
    "                    p_positive += math.log(1 / (sum(self.positive.values()) + len(self.positive)))\n",
    "\n",
    "                if term in self.negative:\n",
    "                    p_negative += math.log(self.negative[term])\n",
    "                else:  # Laplace smoothing for negative\n",
    "                    p_negative += math.log(1 / (sum(self.negative.values()) + len(self.negative)))\n",
    "\n",
    "            classification[id] = 1 if p_positive > p_negative else 0\n",
    "\n",
    "        return classification\n",
    "    \n",
    "\n",
    "    def evaluate(self, classification, scores):\n",
    "        correct = 0\n",
    "\n",
    "        for id, score in scores.items():\n",
    "            if classification[id] == score:\n",
    "                correct += 1\n",
    "\n",
    "        return correct / len(scores.keys())\n",
    "\n",
    "\n",
    "classifier_tfidf = NaiveBayesClassifier()\n",
    "classifier_tfidf.train_tfidf(train_tfidf, train_scores)\n",
    "classification_tfidf = classifier_tfidf.predict(validation_reviews)\n",
    "accuracy_tfidf = classifier_tfidf.evaluate(classification_tfidf, validation_scores)\n",
    "print(f'accuracy for validation set: {accuracy_tfidf}')\n",
    "\n",
    "# predict test set\n",
    "classification_tfidf = classifier_tfidf.predict(test_reviews)\n",
    "accuracy_tfidf = classifier_tfidf.evaluate(classification_tfidf, test_scores)\n",
    "print(f'accuracy for test set: {accuracy_tfidf}')\n",
    "\n",
    "# classifier_ppmi = NaiveBayesClassifier()\n",
    "# classifier_ppmi.train_ppmi(train_ppmi, train_scores, train_reviews)\n",
    "# classification_ppmi = classifier_ppmi.predict(validation_reviews)\n",
    "# accuracy_ppmi = classifier_ppmi.evaluate(classification_ppmi, validation_scores)\n",
    "# print(f'accuracy for validation set: {accuracy_ppmi}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sci-kit naive bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sparse matrix (column of all terms, row of all reviews) \n",
    "\n",
    "# all_terms contain all the terms in the entire dataset, we want a sparse matrix so we need to initialize each row's column with all the terms\n",
    "\n",
    "# review is a dictionary of {id : {term : tfidf}}\n",
    "def create_sparse_matrix(all_terms, reviews):\n",
    "    sparse_matrix = {}\n",
    "\n",
    "    for id, review in reviews.items():\n",
    "        sparse_matrix[id] = {}\n",
    "        for term in all_terms:\n",
    "            sparse_matrix[id][term] = 0\n",
    "\n",
    "    # fill in the existing tfidf values\n",
    "    for id, review in reviews.items():\n",
    "        for term, tfidf in review.items():\n",
    "            sparse_matrix[id][term] = tfidf\n",
    "\n",
    "    return sparse_matrix\n",
    "\n",
    "\n",
    "# get TF-IDF values for validation and test sets\n",
    "test_doc_freq = calc_df(test_reviews)\n",
    "test_inverse_doc_freq = calc_idf(test_doc_freq, len(test_reviews))\n",
    "test_tfidf = calc_tfidf(calc_normalized_tf(test_reviews), test_inverse_doc_freq)\n",
    "\n",
    "validation_doc_freq = calc_df(validation_reviews)\n",
    "validation_inverse_doc_freq = calc_idf(validation_doc_freq, len(validation_reviews))\n",
    "validation_tfidf = calc_tfidf(calc_normalized_tf(validation_reviews), validation_inverse_doc_freq)\n",
    "\n",
    "# create sparse matrix\n",
    "train_sparse_matrix = create_sparse_matrix(all_terms, train_tfidf)\n",
    "validation_sparse_matrix = create_sparse_matrix(all_terms, validation_tfidf)\n",
    "test_sparse_matrix = create_sparse_matrix(all_terms, test_tfidf)\n",
    "\n",
    "# convert to numpy array\n",
    "train_sparse_matrix = np.array([list(review.values()) for review in train_sparse_matrix.values()])\n",
    "validation_sparse_matrix = np.array([list(review.values()) for review in validation_sparse_matrix.values()])\n",
    "test_sparse_matrix = np.array([list(review.values()) for review in test_sparse_matrix.values()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for MultinomialNB on validation set: 0.79625\n",
      "accuracy for MultinomialNB on test set: 0.8\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "train_scores = list(train_scores.values())\n",
    "validation_scores = list(validation_scores.values())\n",
    "test_scores = list(test_scores.values())\n",
    "\n",
    "# train sklearn model\n",
    "multinomial_nb = MultinomialNB()\n",
    "multinomial_nb.fit(train_sparse_matrix, train_scores)\n",
    "\n",
    "\n",
    "# prediction for validation set\n",
    "sklearn_classification = multinomial_nb.predict(validation_sparse_matrix)\n",
    "sklearn_accuracy = multinomial_nb.score(validation_sparse_matrix, validation_scores)\n",
    "print(f'accuracy for MultinomialNB on validation set: {sklearn_accuracy}')\n",
    "\n",
    "\n",
    "# prediction for test set\n",
    "sklearn_classification = multinomial_nb.predict(test_sparse_matrix)\n",
    "sklearn_accuracy = multinomial_nb.score(test_sparse_matrix, test_scores)\n",
    "print(f'accuracy for MultinomialNB on test set: {sklearn_accuracy}') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for logistic regression 2 on validation set: 0.845\n",
      "accuracy for logistic regression 2 on test set: 0.875\n"
     ]
    }
   ],
   "source": [
    "# logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# logistic_regression = LogisticRegression(penalty='none', fit_intercept=True, tol=1e-8)\n",
    "logistic_regression = LogisticRegression()\n",
    "logistic_regression.fit(train_sparse_matrix, train_scores)\n",
    "\n",
    "logistic_regression_2 = LogisticRegression(penalty='none', fit_intercept=True, tol=1e-8)\n",
    "logistic_regression_2.fit(train_sparse_matrix, train_scores)\n",
    "\n",
    "logistic_regression_3 = LogisticRegression(penalty='none', fit_intercept=False, tol=1e-8)\n",
    "logistic_regression_3.fit(train_sparse_matrix, train_scores)\n",
    "\n",
    "logistic_regression_4 = LogisticRegression(penalty='l2', fit_intercept=False, tol=1e-8)\n",
    "logistic_regression_4.fit(train_sparse_matrix, train_scores)\n",
    "\n",
    "logistic_regression_5 = LogisticRegression(penalty='l2', fit_intercept=True, tol=1e-8)\n",
    "logistic_regression_5.fit(train_sparse_matrix, train_scores)\n",
    "\n",
    "\n",
    "# prediction for validtion set\n",
    "# logistic_regression_classification = logistic_regression.predict(validation_sparse_matrix)\n",
    "# logistic_regression_accuracy = logistic_regression.score(validation_sparse_matrix, validation_scores)\n",
    "# print(f'accuracy for logistic regression on validation set: {logistic_regression_accuracy}')\n",
    "\n",
    "logistic_regression_classification = logistic_regression_2.predict(validation_sparse_matrix)\n",
    "logistic_regression_accuracy = logistic_regression_2.score(validation_sparse_matrix, validation_scores)\n",
    "print(f'accuracy for logistic regression 2 on validation set: {logistic_regression_accuracy}')\n",
    "\n",
    "# logistic_regression_classification = logistic_regression_3.predict(validation_sparse_matrix)\n",
    "# logistic_regression_accuracy = logistic_regression_3.score(validation_sparse_matrix, validation_scores)\n",
    "# print(f'accuracy for logistic regression 3 on validation set: {logistic_regression_accuracy}')\n",
    "\n",
    "# logistic_regression_classification = logistic_regression_4.predict(validation_sparse_matrix)\n",
    "# logistic_regression_accuracy = logistic_regression_4.score(validation_sparse_matrix, validation_scores)\n",
    "# print(f'accuracy for logistic regression 4 on validation set: {logistic_regression_accuracy}')\n",
    "\n",
    "# logistic_regression_classification = logistic_regression_5.predict(validation_sparse_matrix)\n",
    "# logistic_regression_accuracy = logistic_regression_5.score(validation_sparse_matrix, validation_scores)\n",
    "# print(f'accuracy for logistic regression 5 on validation set: {logistic_regression_accuracy}')\n",
    "\n",
    "\n",
    "\n",
    "# prediction for test set\n",
    "# logistic_regression_classification = logistic_regression.predict(test_sparse_matrix)\n",
    "# logistic_regression_accuracy = logistic_regression.score(test_sparse_matrix, test_scores)\n",
    "# print(f'accuracy for logistic regression on test set: {logistic_regression_accuracy}') \n",
    "\n",
    "logistic_regression_classification = logistic_regression_2.predict(test_sparse_matrix)\n",
    "logistic_regression_accuracy = logistic_regression_2.score(test_sparse_matrix, test_scores)\n",
    "print(f'accuracy for logistic regression 2 on test set: {logistic_regression_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for linear svc on validation set: 0.835\n",
      "accuracy for linear svc 2 on validation set: 0.835\n",
      "accuracy for linear svc 3 on validation set: 0.82\n",
      "accuracy for linear svc 4 on validation set: 0.83625\n",
      "accuracy for linear svc 5 on validation set: 0.83125\n"
     ]
    }
   ],
   "source": [
    "# svm\n",
    "from sklearn import svm\n",
    "\n",
    "# linear svc\n",
    "linear_svc = svm.LinearSVC(max_iter=10000)\n",
    "linear_svc.fit(train_sparse_matrix, train_scores)\n",
    "\n",
    "linear_svc_2 = svm.LinearSVC(max_iter=10000, penalty='l2', fit_intercept=True, dual=True)\n",
    "linear_svc_2.fit(train_sparse_matrix, train_scores)\n",
    "\n",
    "linear_svc_3 = svm.LinearSVC(max_iter=10000, penalty='l1', dual=False)\n",
    "linear_svc_3.fit(train_sparse_matrix, train_scores)\n",
    "\n",
    "linear_svc_4 = svm.LinearSVC(max_iter=10000, dual=True, fit_intercept=False)\n",
    "linear_svc_4.fit(train_sparse_matrix, train_scores)\n",
    "\n",
    "linear_svc_5 = svm.LinearSVC(max_iter=10000, penalty='l2', dual=False)\n",
    "linear_svc_5.fit(train_sparse_matrix, train_scores)\n",
    "\n",
    "# prediction for validation set\n",
    "linear_svc_classification = linear_svc.predict(validation_sparse_matrix)\n",
    "linear_svc_accuracy = linear_svc.score(validation_sparse_matrix, validation_scores)\n",
    "print(f'accuracy for linear svc on validation set: {linear_svc_accuracy}')\n",
    "\n",
    "linear_svc_classification = linear_svc_2.predict(validation_sparse_matrix)\n",
    "linear_svc_accuracy = linear_svc_2.score(validation_sparse_matrix, validation_scores)\n",
    "print(f'accuracy for linear svc 2 on validation set: {linear_svc_accuracy}')\n",
    "\n",
    "linear_svc_classification = linear_svc_3.predict(validation_sparse_matrix)\n",
    "linear_svc_accuracy = linear_svc_3.score(validation_sparse_matrix, validation_scores)\n",
    "print(f'accuracy for linear svc 3 on validation set: {linear_svc_accuracy}')\n",
    "\n",
    "linear_svc_classification = linear_svc_4.predict(validation_sparse_matrix)\n",
    "linear_svc_accuracy = linear_svc_4.score(validation_sparse_matrix, validation_scores)\n",
    "print(f'accuracy for linear svc 4 on validation set: {linear_svc_accuracy}')\n",
    "\n",
    "linear_svc_classification = linear_svc_5.predict(validation_sparse_matrix)\n",
    "linear_svc_accuracy = linear_svc_5.score(validation_sparse_matrix, validation_scores)\n",
    "print(f'accuracy for linear svc 5 on validation set: {linear_svc_accuracy}')\n",
    "\n",
    "# prediction for test set\n",
    "# linear_svc_classification = linear_svc.predict(test_sparse_matrix)\n",
    "# linear_svc_accuracy = linear_svc.score(test_sparse_matrix, test_scores)\n",
    "# print(f'accuracy for linear svc on test set: {linear_svc_accuracy}') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
